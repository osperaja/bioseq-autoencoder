{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bioseq Autoencoder Prototype\n",
    "\n",
    "This notebook prototypes an autoencoder as a proof of concept for denoising genomic sequences.\n",
    "The denoising is achieved by training the autoencoder on noisy $k$-mers, where $k$ is a parameter that defines the length of the $k$-mers.\n",
    "Denoising can be helpful in genome assembly processes, especially since the input data may contain *single nucleotide polymorphisms*, which are single base changes, or sequencing errors.\n",
    "\n",
    "In the context of De-Bruijn graph construction, the autoencoder can denoise $k$-mers before graph assembly, reducing bubbles and improving overall assembly quality.\n",
    "Bubbles (structures in a De-Bruijn graph formed by divergent and convergent paths between the same nodes) are caused by sequencing errors or heterozygosity.\n",
    "This is resulting in local graph ambiguities that hinder accurate genome assembly, due to disrupting the formation of Eulerian paths.\n",
    "This could also help in identifying true low-frequency variants, such as somatic mutations in cancer genomes.\n",
    "\n",
    "This is a thesis proposal, however, not a full thesis proposal.\n",
    "It is a first step towards understanding where current research may be heading."
   ],
   "id": "86213cd09f3d700e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-15T00:33:19.834119Z",
     "start_time": "2025-08-15T00:33:17.824580Z"
    }
   },
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install scikit-learn"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: filelock in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:33:21.837750Z",
     "start_time": "2025-08-15T00:33:19.922968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.__version__, torch.version.cuda, torch.backends.cudnn.version(), device)"
   ],
   "id": "dbd08273569c832c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126 12.6 91002 cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " Firstly, I will download a FASTA file containing the _Escherichia coli_ genome from NCBI.\n",
    "The file is compressed in gzip format, so I will use the `gzip` module to read it.\n",
    "The `Bio.SeqIO` module from Biopython will be used to parse the FASTA file."
   ],
   "id": "54842dee52cebd0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:33:22.904078Z",
     "start_time": "2025-08-15T00:33:21.851735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz\"\n",
    "urllib.request.urlretrieve(url, \"ecoli.fna.gz\")\n",
    "\n",
    "with gzip.open(\"ecoli.fna.gz\", \"rt\") as handle:\n",
    "    records = list(SeqIO.parse(handle, \"fasta\"))\n",
    "\n",
    "print(f\"number of sequences: {len(records)}\")\n",
    "print(f\"first record ID: {records[0].id}\")\n",
    "print(f\"length: {len(records[0].seq)} bases\")\n",
    "print(f\"first 100 nucleotides:\\n{records[0].seq[:100]}\")"
   ],
   "id": "7620d9e48300a786",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences: 1\n",
      "first record ID: NC_000913.3\n",
      "length: 4641652 bases\n",
      "first 100 nucleotides:\n",
      "AGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAAAT\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Secondly, I will define functions to extract $k$-mers from the sequence and add salt-and-pepper-ish noise to the sequence.",
   "id": "b8fb73cee094e64a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:33:22.909976Z",
     "start_time": "2025-08-15T00:33:22.907398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_kmers(sequence, k):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "def add_noise(sequence, noise_level=0.1):\n",
    "    noisy = list(sequence)\n",
    "    for i in range(len(sequence)):\n",
    "        if random.random() < noise_level:\n",
    "            noisy[i] = random.choice([n for n in \"ACGT\" if n != sequence[i]])\n",
    "    return \"\".join(noisy)"
   ],
   "id": "776250a693ca11cb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I will define functions to convert $k$-mers into one-hot encoded tensors.\n",
    "Each nucleotide will be represented as a vector of length $4$, where the position corresponding to the nucleotide is set to `1` and all other positions are set to `0`."
   ],
   "id": "ed2033fd0081e264"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:33:22.922741Z",
     "start_time": "2025-08-15T00:33:22.920520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot(kmer):\n",
    "    mapping = {\"A\": [1, 0, 0, 0], \"C\": [0, 1, 0, 0], \"G\": [0, 0, 1, 0], \"T\": [0, 0, 0, 1]}\n",
    "    return torch.tensor(np.array([mapping[nucleotide] for nucleotide in kmer]), dtype=torch.float32)\n",
    "\n",
    "def one_hot_batch(kmers):\n",
    "    return torch.stack([one_hot(kmer) for kmer in kmers])"
   ],
   "id": "7e90e3646aaec4f5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, I will extract $k$-mers from the first record in the FASTA file and create a noisy version of the sequence.\n",
    "I set $k = 31$, which is a common choice for genomic sequences.\n",
    "After that, I will create an $X\\%$-subset of the $k$-mers for training and validation."
   ],
   "id": "2cdaa98b52bc4c77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:34:53.840834Z",
     "start_time": "2025-08-15T00:33:22.967056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 31\n",
    "sequence = str(records[0].seq)\n",
    "noisy_sequence = add_noise(sequence)\n",
    "\n",
    "clean_kmers = get_kmers(sequence, k)\n",
    "noisy_kmers = get_kmers(noisy_sequence, k)\n",
    "\n",
    "subset_fraction = 0.8\n",
    "total_kmers = len(clean_kmers)\n",
    "subset_size = int(subset_fraction * total_kmers)\n",
    "\n",
    "subset_indices = np.random.choice(total_kmers, size=subset_size, replace=False)\n",
    "\n",
    "x_clean_subset = one_hot_batch([clean_kmers[i] for i in subset_indices])\n",
    "x_noisy_subset = one_hot_batch([noisy_kmers[i] for i in subset_indices])\n",
    "x_clean_train, x_clean_val, x_noisy_train, x_noisy_val = train_test_split(\n",
    "    x_clean_subset, x_noisy_subset, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(x_noisy_train, x_clean_train)\n",
    "val_dataset = TensorDataset(x_noisy_val, x_clean_val)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ],
   "id": "bf80543fd4dae817",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, I will define the autoencoder model for $k$-mers.",
   "id": "da38080cb1f21704"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:34:53.888577Z",
     "start_time": "2025-08-15T00:34:53.884822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KMerAutoenc(nn.Module):\n",
    "    def __init__(self, k, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(k * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, k * 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        out = out.view(-1, self.k, 4)\n",
    "        return out"
   ],
   "id": "9ca46382a509b147",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Why don't we create an instance of the model and define the loss function and optimiser?",
   "id": "df0892d9c4dbe69f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:34:54.746447Z",
     "start_time": "2025-08-15T00:34:53.951333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = KMerAutoenc(k).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "e3772a259390438a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I am using the `MSELoss` loss function.\n",
    "As a quality metric, I will use the Hamming distance between the predicted and target $k$-mers."
   ],
   "id": "e82897abebff422"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T00:34:54.772885Z",
     "start_time": "2025-08-15T00:34:54.758192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hamming_distance(output, target):\n",
    "    pred = output.argmax(dim=2)\n",
    "    true = target.argmax(dim=2)\n",
    "    return (pred != true).float().mean().item()\n",
    "\n",
    "def hamming_quality(hamming):\n",
    "    if hamming <= 0.05:\n",
    "        return \"very good\"\n",
    "    elif hamming <= 0.2:\n",
    "        return \"acceptable\"\n",
    "    elif hamming <= 0.5:\n",
    "        return \"poor\"\n",
    "    else:\n",
    "        return \"terrible\"\n",
    "\n",
    "def train(model, train_loader, val_loader, optimiser, criterion, epochs=10):\n",
    "    train_losses, val_losses, train_hamming, val_hamming = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_ham = 0\n",
    "        for x_noisy, x_clean in train_loader:\n",
    "            x_noisy, x_clean = x_noisy.to(device), x_clean.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            output = model(x_noisy)\n",
    "            loss = criterion(output, x_clean)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item() * x_noisy.size(0)\n",
    "            running_ham += hamming_distance(output, x_clean) * x_noisy.size(0)\n",
    "\n",
    "        train_loss_epoch = running_loss / len(train_loader.dataset)\n",
    "        train_ham_epoch = running_ham / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        train_hamming.append(train_ham_epoch)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_total = 0\n",
    "        val_ham_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x_noisy, x_clean in val_loader:\n",
    "                x_noisy, x_clean = x_noisy.to(device), x_clean.to(device)\n",
    "                val_output = model(x_noisy)\n",
    "                val_loss_total += criterion(val_output, x_clean).item() * x_noisy.size(0)\n",
    "                val_ham_total += hamming_distance(val_output, x_clean) * x_noisy.size(0)\n",
    "\n",
    "        val_loss_epoch = val_loss_total / len(val_loader.dataset)\n",
    "        val_ham_epoch = val_ham_total / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        val_hamming.append(val_ham_epoch)\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{epochs} | \"\n",
    "              f\"train loss: {train_loss_epoch:.4f} | val loss: {val_loss_epoch:.4f} | \"\n",
    "              f\"train hamming: {train_ham_epoch:.4f} ({hamming_quality(train_ham_epoch)}) | \"\n",
    "              f\"val hamming: {val_ham_epoch:.4f} ({hamming_quality(val_ham_epoch)})\")\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='train loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='val loss')\n",
    "    plt.xlabel('epoch'); plt.ylabel('loss'); plt.title('loss'); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(1, epochs+1), train_hamming, label='train hamming')\n",
    "    plt.plot(range(1, epochs+1), val_hamming, label='val hamming')\n",
    "    plt.xlabel('epoch'); plt.ylabel('hamming distance'); plt.title('hamming distance'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "f86b0ef3d2686149",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's train the model.",
   "id": "7af57ee39d5eb002"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-15T00:34:54.810856Z"
    }
   },
   "cell_type": "code",
   "source": "train(model, train_loader, val_loader, optimiser, criterion, epochs=50)\n",
   "id": "bfcb3208aa0d0676",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/50 | train loss: 0.2416 | val loss: 0.2034 | train hamming: 0.1498 (acceptable) | val hamming: 0.1027 (acceptable)\n",
      "epoch 2/50 | train loss: 0.1994 | val loss: 0.2006 | train hamming: 0.1010 (acceptable) | val hamming: 0.1018 (acceptable)\n",
      "epoch 3/50 | train loss: 0.1966 | val loss: 0.1976 | train hamming: 0.1004 (acceptable) | val hamming: 0.1022 (acceptable)\n",
      "epoch 4/50 | train loss: 0.1944 | val loss: 0.1988 | train hamming: 0.1002 (acceptable) | val hamming: 0.1049 (acceptable)\n",
      "epoch 5/50 | train loss: 0.1926 | val loss: 0.1948 | train hamming: 0.1002 (acceptable) | val hamming: 0.1016 (acceptable)\n",
      "epoch 6/50 | train loss: 0.1914 | val loss: 0.1993 | train hamming: 0.1001 (acceptable) | val hamming: 0.1089 (acceptable)\n",
      "epoch 7/50 | train loss: 0.1904 | val loss: 0.1900 | train hamming: 0.1001 (acceptable) | val hamming: 0.1001 (acceptable)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To suffice the proof of concept, the model should regard multiple variants of the same $k$-mer, or more specific, multiple variants of the underlying genome, including low-frequency somatic mutations.",
   "id": "1495ea97c24647ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
