{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bioseq Autoencoder Prototype\n",
    "\n",
    "This notebook prototypes an autoencoder as a proof of concept for denoising genomic sequences.\n",
    "The denoising is achieved by training the autoencoder on noisy $k$-mers, where $k$ is a parameter that defines the length of the $k$-mers.\n",
    "Denoising can be helpful in genome assembly processes, especially since the input data may contain *single nucleotide polymorphisms*, which are single base changes, or sequencing errors.\n",
    "\n",
    "In the context of De-Bruijn graph construction, the autoencoder can denoise $k$-mers before graph assembly, reducing bubbles and improving overall assembly quality.\n",
    "Bubbles (structures in a De-Bruijn graph formed by divergent and convergent paths between the same nodes) are caused by sequencing errors or heterozygosity (at least for what Copilot told me).\n",
    "This is resulting in local graph ambiguities that hinder accurate genome assembly, due to disrupting the formation of Eulerian paths.\n",
    "This could also help in identifying true low-frequency variants, such as somatic mutations in cancer genomes.\n",
    "\n",
    "This is a thesis proposal, however, not a full thesis proposal.\n",
    "It is a first step towards understanding where current research may be heading."
   ],
   "id": "839c0c1bdbc01a2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T13:59:36.289495Z",
     "start_time": "2025-08-16T13:59:34.109018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install scikit-learn"
   ],
   "id": "8fe00ddeb28d9669",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (0.23.0+cu126)\n",
      "Requirement already satisfied: filelock in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jamil\\pycharmprojects\\bioseq-autoencoder\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T15:30:25.891287Z",
     "start_time": "2025-08-16T15:30:25.888457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.__version__, torch.version.cuda, torch.backends.cudnn.version(), device)"
   ],
   "id": "b41ac8f246ecffd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126 12.6 91002 cuda\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Firstly, I will download a FASTA file containing the _Escherichia coli K-12 MG1655_ genome from NCBI.\n",
    "The file is compressed in _gzip_ format, so I will use the `gzip` module to read it.\n",
    "The `Bio.SeqIO` module from Biopython will be used to parse the FASTA file."
   ],
   "id": "ae571edcd09dae31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T13:59:41.055170Z",
     "start_time": "2025-08-16T13:59:38.842378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "assemblies = [\n",
    "    (\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz\", \"GCF_000005845.2_ASM584v2_genomic.fna.gz\"),\n",
    "    (\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/008/865/GCF_000008865.2_ASM886v2/GCF_000008865.2_ASM886v2_genomic.fna.gz\", \"GCF_000008865.2_ASM886v2_genomic.fna.gz\"),\n",
    "    # (\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/012/525/GCF_000012525.1_ASM1252v1/GCF_000012525.1_ASM1252v1_genomic.fna.gz\", \"GCF_000012525.1_ASM1252v1_genomic.fna.gz\"),\n",
    "    # (\"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/013/305/GCF_000013305.1_ASM1330v1/GCF_000013305.1_ASM1330v1_genomic.fna.gz\", \"GCF_000013305.1_ASM1330v1_genomic.fna.gz\"),\n",
    "]\n",
    "\n",
    "for asm in assemblies:\n",
    "    print(f\"Downloading {asm[1]} from {asm[0]}\")\n",
    "    urllib.request.urlretrieve(asm[0], asm[1])\n",
    "\n",
    "full_sequence = \"\"\n",
    "\n",
    "for _, filename in assemblies:\n",
    "    with gzip.open(filename, \"rt\") as handle:\n",
    "        records = list(SeqIO.parse(handle, \"fasta\"))\n",
    "        for record in records:\n",
    "            full_sequence += str(record.seq)\n",
    "\n",
    "print(f\"concatenated sequence length: {len(full_sequence)}\")"
   ],
   "id": "78d7403badbd8535",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GCF_000005845.2_ASM584v2_genomic.fna.gz from https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz\n",
      "Downloading GCF_000008865.2_ASM886v2_genomic.fna.gz from https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/008/865/GCF_000008865.2_ASM886v2/GCF_000008865.2_ASM886v2_genomic.fna.gz\n",
      "concatenated sequence length: 10236257\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Secondly, I will define functions to extract $k$-mers from the sequence and add salt-and-pepper-ish noise to the sequence.",
   "id": "7f6e5ac8a65be73d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T13:59:41.064749Z",
     "start_time": "2025-08-16T13:59:41.062442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_kmers(sequence, k):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "def add_noise(kmer, noise_level=0.1):\n",
    "    noisy = list(kmer)\n",
    "    for i in range(len(kmer)):\n",
    "        if random.random() < noise_level:\n",
    "            noisy[i] = random.choice([n for n in \"ACGT\" if n != kmer[i]])\n",
    "    return \"\".join(noisy)"
   ],
   "id": "4acf47966e4c1af1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I will define functions to convert $k$-mers into one-hot encoded tensors.\n",
    "Each nucleotide will be represented as a vector of length $4$, where the position corresponding to the nucleotide is set to `1` and all other positions are set to `0`."
   ],
   "id": "680ab52921208099"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T13:59:41.090282Z",
     "start_time": "2025-08-16T13:59:41.087921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot(kmer):\n",
    "    mapping = {\"A\": [1, 0, 0, 0], \"C\": [0, 1, 0, 0], \"G\": [0, 0, 1, 0], \"T\": [0, 0, 0, 1]}\n",
    "    return torch.tensor(np.array([mapping[nucleotide] for nucleotide in kmer]), dtype=torch.float32)\n",
    "\n",
    "def one_hot_batch(kmers):\n",
    "    return torch.stack([one_hot(kmer) for kmer in kmers])"
   ],
   "id": "8df746605fd1bf8f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, I will extract $k$-mers from the record in the FASTA file and create a noisy version of the sequence.\n",
    "I set $k = 31$, which is a common choice for genomic sequences.\n",
    "After that, I will create an $X\\%$-subset of the $k$-mers for training and validation.\n",
    "I will call this the _f_-phase (flooding phase) (you may observe the RAM utilisation during execution to see why)."
   ],
   "id": "c2cbbefcabe748f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T14:16:17.330219Z",
     "start_time": "2025-08-16T14:12:32.887160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "noise_floor = 0.1\n",
    "k = 31\n",
    "\n",
    "clean_kmers, noisy_kmers = get_kmers(full_sequence, k), get_kmers(full_sequence, k)\n",
    "noisy_kmers = [add_noise(kmer, noise_floor) for kmer in clean_kmers]\n",
    "\n",
    "subset_fraction = 0.8\n",
    "total_kmers = len(clean_kmers)\n",
    "subset_size = int(subset_fraction * total_kmers)\n",
    "\n",
    "subset_indices = np.random.choice(total_kmers, size=subset_size, replace=False)\n",
    "\n",
    "x_clean_subset = one_hot_batch([clean_kmers[i] for i in subset_indices])\n",
    "x_noisy_subset = one_hot_batch([noisy_kmers[i] for i in subset_indices])\n",
    "x_clean_train, x_clean_val, x_noisy_train, x_noisy_val = train_test_split(\n",
    "    x_clean_subset, x_noisy_subset, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(x_noisy_train, x_clean_train)\n",
    "val_dataset = TensorDataset(x_noisy_val, x_clean_val)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ],
   "id": "f456f6c0f51336bb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, I will define the autoencoder model for $k$-mers.",
   "id": "5adab84671527fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T14:16:35.745806Z",
     "start_time": "2025-08-16T14:16:35.742555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KMerAutoenc(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(k * 4, int(k * 3.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(k * 3.5), k * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(k * 3, int(k * 2.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(int(k * 2.5)),\n",
    "            nn.Linear(int(k * 2.5), k * 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(k * 2, int(k * 2.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(k * 2.5), k * 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(k * 3, int(k * 3.5)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(k * 3.5), k * 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        out = out.view(-1, self.k, 4)\n",
    "        return out"
   ],
   "id": "4d2a89e209b46449",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Why don't we create an instance of the model and define the loss function and optimiser?",
   "id": "d44e6978f47ebfaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T15:30:38.147787Z",
     "start_time": "2025-08-16T15:30:38.143601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_fcffae = KMerAutoenc(k).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model_fcffae.parameters(), lr=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimiser, mode='min', factor=0.1, patience=3)"
   ],
   "id": "5aa216737a56b35",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I am using the `CrossEntropyLoss` loss function, for multi-label classification, since each nucleotide in the $k$-mer is represented as a separate label.\n",
    "As a quality metric, I will use the Hamming distance between the predicted and target $k$-mers."
   ],
   "id": "6285158bd36bfa43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T15:30:39.770129Z",
     "start_time": "2025-08-16T15:30:39.762832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hamming_distance(output, target):\n",
    "    pred = output.argmax(dim=2)\n",
    "    true = target.argmax(dim=2)\n",
    "    return (pred != true).float().mean().item()\n",
    "\n",
    "def hamming_quality(hamming):\n",
    "    if hamming <= 0.5 * noise_floor:\n",
    "        return \"good denoise\"\n",
    "    elif hamming < noise_floor:\n",
    "        return \"denoise\"\n",
    "    else:\n",
    "        return \"no denoise\"\n",
    "\n",
    "def train(model, train_loader, val_loader, optimiser, criterion, epochs=10):\n",
    "    train_losses, val_losses, train_hamming, val_hamming = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        running_ham = 0\n",
    "        for x_noisy, x_clean in train_loader:\n",
    "            x_noisy, x_clean = x_noisy.to(device), x_clean.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            output = model(x_noisy)\n",
    "            targets = torch.argmax(x_clean, dim=2)\n",
    "            loss = criterion(output.view(-1, 4), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item() * x_noisy.size(0)\n",
    "            running_ham += hamming_distance(output, x_clean) * x_noisy.size(0)\n",
    "\n",
    "        train_loss_epoch = running_loss / len(train_loader.dataset)\n",
    "        train_ham_epoch = running_ham / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        train_hamming.append(train_ham_epoch)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_total = 0\n",
    "        val_ham_total = 0\n",
    "        with torch.no_grad():\n",
    "            for x_noisy, x_clean in val_loader:\n",
    "                x_noisy, x_clean = x_noisy.to(device), x_clean.to(device)\n",
    "                val_output = model(x_noisy)\n",
    "                val_loss_total += criterion(val_output, x_clean).item() * x_noisy.size(0)\n",
    "                val_ham_total += hamming_distance(val_output, x_clean) * x_noisy.size(0)\n",
    "\n",
    "        val_loss_epoch = val_loss_total / len(val_loader.dataset)\n",
    "        val_ham_epoch = val_ham_total / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss_epoch)\n",
    "        val_hamming.append(val_ham_epoch)\n",
    "        scheduler.step(val_loss_epoch)\n",
    "\n",
    "        print(f\"epoch {epoch+1}/{epochs} | \"\n",
    "              f\"train loss: {train_loss_epoch:.4f} | val loss: {val_loss_epoch:.4f} | \"\n",
    "              f\"train hamming: {train_ham_epoch:.4f} ({hamming_quality(train_ham_epoch)}) | \"\n",
    "              f\"val hamming: {val_ham_epoch:.4f} ({hamming_quality(val_ham_epoch)})\")\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='train loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='val loss')\n",
    "    plt.xlabel('epoch'); plt.ylabel('loss'); plt.title('loss'); plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(1, epochs+1), train_hamming, label='train hamming')\n",
    "    plt.plot(range(1, epochs+1), val_hamming, label='val hamming')\n",
    "    plt.axhline(y=0.5 * noise_floor, color='red', linestyle='--', label='good denoise threshold')\n",
    "    plt.axhline(y=noise_floor, color='orange', linestyle='--', label='denoise threshold')\n",
    "    plt.xlabel('epoch'); plt.ylabel('hamming distance'); plt.title('hamming distance'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "8eaf418cd37b65f7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's train the model.",
   "id": "8a45360a476a40b6"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-16T15:30:42.088517Z"
    }
   },
   "cell_type": "code",
   "source": "train(model_fcffae, train_loader, val_loader, optimiser, criterion, epochs=20)\n",
   "id": "70746337dcf81617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20 | train loss: 1.1442 | val loss: 26.6014 | train hamming: 0.5225 (no denoise) | val hamming: 0.5066 (no denoise)\n",
      "epoch 2/20 | train loss: 1.1197 | val loss: 27.5372 | train hamming: 0.5114 (no denoise) | val hamming: 0.5028 (no denoise)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To suffice the proof of concept, the model should regard multiple variants of the same $k$-mer.\n",
    "Or more specific in regard to the thesis topic proposal, multiple variants of a more complex genome, including low-frequency somatic mutations."
   ],
   "id": "dce4506b1185f22f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
